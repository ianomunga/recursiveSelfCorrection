{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recursiveSelfCorrection - Ian Omung'a.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1GcVvk867B2KuN6bEzR2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianomunga/recursiveSelfCorrection/blob/main/recursiveSelfCorrection_Ian_Omung'a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Abstract\n",
        "Conventional error correction is based on implementations of dedicated error handling sub-systems that serialize all the ways the developers and designers of the main system envision runtime error could occur. In best case scenarios where these error handling subsystems need not be utilized at all, they serve a vestigial purpose, existing solely for the off chance that they may be needed in very specific scenarios. When necessary, the capacity of these error handling subsystems to sufficiently handle errors is limited by whatever scenario was envisioned during the main system’s creation. Implementing quadratic loss functions as embedded, recursive elements of error handling that can be dynamically updated at runtime within robust self correction algorithms serves as a progressively improving, automated framework for error handling that not only need not be constantly maintained or tweaked based on inferred problematic scenarios but also as one that gets better overtime. Foundational formalizations of this algorithm show the logical structure of the self correction framework and one such implementation pathway to achieve it. The modularity of the algorithm to incorporate varied loss functions is also exhibited through its alternative implementation using the Mean Absolute Error loss function. \n",
        "\n",
        "\n",
        "#Introduction\n",
        "Error handling as it is implemented now is heavily dependent on hypothetical scenarios envisioned during the design and creation of information processing systems and as such, is not able to handle data sources prone to high error occurrence in widely varied ways that are hard to predict. Iterative self correction algorithms that work through dynamic quadratic loss functions are seen to be a source of constant improvement in a data-driven process where the human practitioners who calibrate it can do so from an informed standpoint. In artificial intelligence systems, the dynamic updating of the parametres of the self-correction algorithm can be automated by programming self-attention mechanisms within the artificial intelligence systems to update the loss functions at runtime whenever predicted values are divergent from the ground truth values on which training is being performed. \n",
        "Quadratic loss functions suitability for optimizing the distance of displacement away from the erroneous points in the vector space of solutions for information processing systems emerges from the heavy penalization they effect when such errors occur. Since the factors in the function are only evaluated upto square factors [by a power of 2], the mathematical penalizations and subsequent transformations away from erroneous vector regions are fairly straightforward to interpret and replicate if need be. Furthermore, the versatility of the quadratic loss function through scaling up by scalar factors means that error corrections of higher magnitudes can be executed all at once without the need for multiple iterations within the information processing system, thereby lessening processor time consumed on error correction in aggregate. Iterating through multiple generations of automated error correction can therefore be seen to function in the same manner as an evolutionary algorithmic process; selecting for solutions with the least error and eliminating ones with errors from the vector space so as to create successively better information processing standards. \n",
        "\n",
        "The quadratic loss function within the iterative self-correction algorithm is defined as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "nW6XUHF8S6BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**$\\frac{\\sum_{i=1}^n(yᵢ-ŷ_i)^2}{n}$**\n",
        "\n",
        "Where n\t=\tnumber of training examples \n",
        "\t    i\t    =\ti<sup>th</sup>\n",
        "  <br>\n",
        "\t   y(i)\t=\tground truth label for i<sup>th</sup> training example\n",
        "  <br>\n",
        "\t   y(i)\t=\tmodel’s prediction for i<sup>th</sup> training example\n",
        "\n",
        "Holding the above quadratic loss function as true, the following is the algorithm’s code implemented in Python with the NumPy<sup>6</sup>, a Python mathematical library:"
      ],
      "metadata": {
        "id": "TOLeD4Wcl_BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\"\"\"all imports here in a separate code cell to maintain\n",
        "logical and flow and be unobtrusive if need arises for\n",
        "subsequent imports\"\"\""
      ],
      "metadata": {
        "id": "izmq6T3mbKSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class r_correct(Exception):\n",
        "    \"\"\"\n",
        "    The recursive self correction algorithm is implemented by\n",
        "    defining the r_correct class that inherits runtime exception handling  \n",
        "    capabilities from the Exception class\n",
        "    \"\"\"\n",
        "    def __init__(self, r_correct, message=\"An Error has been self-corrected\"):\n",
        "        self.r_correct = r_correct\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "    def qLoss(predictions, targets):\n",
        "      \"\"\"implement modular optimization function that will in turn be \n",
        "      called in case of any errors in the neural net's execution.\n",
        "      The default optimization function for r_correct algorithm\n",
        "      is the quadratic loss function, called qLoss\n",
        "      \"\"\"\n",
        "      differences = predictions - targets\n",
        "      differences_squared = differences ** 2\n",
        "      mean_of_differences_squared = differences_squared.mean()\n",
        "      qLoss_val = np.sqrt(mean_of_differences_squared)\n",
        "      return qLoss_val\n",
        "      "
      ],
      "metadata": {
        "id": "gPs8n5l4Zvt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jkPq-qmSOk_"
      },
      "outputs": [],
      "source": [
        "y_h = np.array([0.000, 0.365, 0.143])\n",
        "y_true = np.array([0.000, 0.767, 0.469])\n",
        "print(\"d is: \" + str([\"%.8f\" % elem for elem in y_h]))\n",
        "print(\"p is: \" + str([\"%.8f\" % elem for elem in y_true]))\n",
        "qLoss_val = qLoss(y_h, y_true)\n",
        "if y_h.all != y_true.all:\n",
        "  print(\"qLoss error is: \" + str(qLoss_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Contributions\n",
        "*  Faster timelines for improvement for large scale parametre models\n",
        "Large scale parametre models today possess an immense number of parametres, ranging from 1.5 Billion1 parametres to 1.75 Trillion2 parameters. These herculean model sizes mean that error correction within the neural networks can only be effected after source data has been ingested through its data pipelines and processed across the neural networks various parallel layers. Incorporating dynamic self-correction algorithms into the infrastructure of these neural networks at the parametre level cuts enables the identification and necessary correction of errors in real time during processing. This further means that the timelines within which such large scale neural networks can be developed, tested and validated will be much shorter in comparison to neural networks functioning without the iterative self-correcting algorithm.\n",
        "\n",
        "*  High efficiency error correction without the need for human intervention\n",
        "Current error correction processes rely on human practitioners to monitor and test information processing systems like artificial intelligence models during data processing so as to attempt to determine the causal agents of errors within the models’ parametres. In some cases, the causal agents of the errors within the neural networks are completely unknown to the human practitioners who design and operate them3. Using this iterative self-correction algorithm as an automated algorithm for error correction embedded within the hidden layers of the neural networks means that errors created by the neural networks’ yet unknown mathematical operations can be probed, understood and corrected before interfering with the neural networks functionality.\n",
        "\n",
        "*  Formalized self-correcting algorithm that can be changed and adapted for variant use-cases\n",
        "Conventional error correction procedures are limited to post-processing use to examine possible causes of flawed functionality and model performance below the expectations of the designers of the neural networks. Contrary to the former approach, this iterative self-correcting algorithm’s error correcting operations are alterable dynamically during the neural network’s processing and can be scaled as needed to whatever factor necessary. \n",
        "\n",
        "*  Modular loss function that can be varied using scalar operations or be replaced entirely without incapacitating the algorithm\n",
        "The self-correcting algorithm proposed herein is versatile in the mode of its usage due to the fact that the quadratic loss function can be alternated entirely for functions that pursue different approaches to the task of optimizing the error correction process. In place of the quadratic loss function, alternative optimization functions like the Mean Absolute Error function can be reliably used within the iterative self-correction algorithm for error correction.\n"
      ],
      "metadata": {
        "id": "jt6YYWZUe53A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusions\n",
        "The algorithm is adaptable for inclusion within large scale projects working on highly variant and unlabelled corpora like texts scrapped from the entirety of the internet where errors occur frequently and need to be corrected quickly\n"
      ],
      "metadata": {
        "id": "F8VYRpG0fUwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Future Work\n",
        "Subsequent work will be done to formalize the algorithm mathematically as an embedded component within the self-attention transformer architecture (Vaswani et. al., 2017) so that attentiveness can be coupled with swift error correction at each juncture\n"
      ],
      "metadata": {
        "id": "8fOIHTRkfW46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Acknowledgements\n",
        "A substantial amount of the mathematical knowledge used in the design of this algorithm has been adapted from the reference book, ‘Mathematics for Machine Learning’, \n"
      ],
      "metadata": {
        "id": "okZKAMH-fd5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "* Solaiman, I., Clark, J. and Brundage, M., 2019. GPT-2: 1.5B Release. [online] OpenAI. Available at: <https://openai.com/blog/gpt-2-1-5b-release/> [Accessed 13 January 2022].\n",
        "* WeChat public platform. 2022. Facing cognition, Zhiyuan Research Institute jointly released a new super-large-scale pre-training model \"Wu Dao·Wenhui\". [online] Available at: <https://mp.weixin.qq.com /s/BUQWZ5EdR19i40GuFofpBg> [Accessed 14 January 2022].\n",
        "* Bathaee, Y., 2018. THE ARTIFICIAL INTELLIGENCE BLACK BOX AND THE FAILURE OF INTENT AND CAUSATION. [online] Jolt.law.harvard.edu. Available at: <https://jolt.law.harvard.edu/assets/articlePDFs/v31/The-Artificial-Intelligence-Black-Box-and-the-Failure-of-Intent-and-Causation-Yavar-Bathaee.pdf> [Accessed 14 January 2022]\n",
        "* Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Gomez, A., Kaiser, L. and Polosukhin, I., 2017. Attention Is All You Need. Arxiv.org, [online] arXiv:1706.03762, p.Abstract. Available at: <https://arxiv.org/abs/1706.03762> [Accessed 13 January 2022].\n",
        "* Deisenroth, M., Faisal, A. and Ong, C., 2020. Mathematics for Machine Learning. 2nd ed. Cambridge: Cambridge University Press, pp.159-163.\n",
        "* Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357–362 (2020)."
      ],
      "metadata": {
        "id": "V5S17fRpfjeW"
      }
    }
  ]
}